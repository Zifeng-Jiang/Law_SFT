{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨æ³•å¾‹ç›¸å…³æ•°æ®é›†å¾®è°ƒ DeepSeek-R1\n",
    "## ä½¿ç”¨unslothä»æœ¬åœ°åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzf/anaconda3/envs/law_sft/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.12: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jzf/models/DeepSeek-R1-Distill-Qwen-14B/ does not have a padding token! Will use pad_token = <|vision_pad|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# åŸºäºunslothåŠ è½½Llamaçš„è’¸é¦æ¨¡å‹\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/jzf/models/DeepSeek-R1-Distill-Qwen-14B/\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ„å»ºpromptæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ®µç­”è¾©çš„è¾©è®ºæé—®ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿é€»è¾‘å’Œå‡†ç¡®çš„å“åº”ã€‚\n",
    "\n",
    "### æŒ‡ä»¤ï¼š\n",
    "ä½ æ˜¯ä¸€ä½åœ¨ä¸­å›½æ³•å¾‹ã€æ¡ˆä¾‹æ¨ç†å’Œåˆ¤å†³æ–¹é¢å…·æœ‰é«˜çº§çŸ¥è¯†çš„æ³•å¾‹ä¸“å®¶ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹æ³•å¾‹é—®é¢˜ã€‚\n",
    "\n",
    "### é—®é¢˜ï¼š\n",
    "{}\n",
    "\n",
    "### å“åº”ï¼š\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶æ—¶ï¼Œå¿…é¡»ç¡®ä¿æ¡ˆä»¶çš„è¯æ®ç¡®å®å……åˆ†ï¼ŒåŒæ—¶è¦æ ¸å®æ¡ˆæƒ…æ˜¯å¦ç¬¦åˆæ³•å¾‹è§„å®šï¼Œæ­¤å¤–è¿˜è¦æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…è¦çš„æ³•å¾‹ç¨‹åºéƒ½å·²ç»å®Œæˆã€‚è¿™åŒ…æ‹¬å¯¹çŠ¯ç½ªäº‹å®çš„ç¡®è®¤ã€è¯æ®çš„å……åˆ†æ€§å’Œåˆæ³•æ€§ï¼Œä»¥åŠæ³•å¾‹é€‚ç”¨çš„å‡†ç¡®æ€§ã€‚è¿™äº›æ­¥éª¤ç¡®ä¿äº†æ¡ˆä»¶çš„å…¬æ­£æ€§å’Œæ³•å¾‹çš„æ­£ç¡®æ‰§è¡Œã€‚\n",
      "\n",
      "### æ€ç»´é“¾ï¼š\n",
      "1. **æ¡ˆä»¶äº‹å®çš„æ ¸å®**ï¼šç¡®è®¤æ¡ˆä»¶ä¸­çš„çŠ¯ç½ªäº‹å®æ˜¯å¦å·²ç»æ˜ç¡®ï¼Œæ˜¯å¦æœ‰è¶³å¤Ÿçš„è¯æ®æ”¯æŒã€‚\n",
      "2. **è¯æ®çš„å……åˆ†æ€§å’Œåˆæ³•æ€§**ï¼šæ£€æŸ¥æ‰€æœ‰çš„è¯æ®æ˜¯å¦ç¬¦åˆæ³•å¾‹è§„å®šï¼Œæ˜¯å¦å……åˆ†æ”¯æŒæŒ‡æ§ã€‚\n",
      "3. **æ³•å¾‹ç¨‹åºçš„å®Œæ•´æ€§**ï¼šç¡®ä¿æ‰€æœ‰ç›¸å…³çš„æ³•å¾‹ç¨‹åºéƒ½å·²ç»æ­£ç¡®æ‰§è¡Œï¼Œæ²¡æœ‰é—æ¼æˆ–é”™è¯¯ã€‚\n",
      "4. **æ³•å¾‹é€‚ç”¨çš„å‡†ç¡®æ€§**ï¼šç¡®è®¤é€‚ç”¨çš„æ³•å¾‹æ˜¯å¦æ­£ç¡®ï¼Œæ˜¯å¦ç¬¦åˆæ¡ˆä»¶çš„å…·ä½“æƒ…å†µã€‚\n",
      "5. **æ¡ˆä»¶çš„æ³•å¾‹æ•ˆæœå’Œç¤¾ä¼šæ•ˆæœ**ï¼šè€ƒè™‘æ¡ˆä»¶çš„å¤„ç†æ˜¯å¦ç¬¦åˆç¤¾ä¼šå…¬æ­£å’Œæ³•å¾‹æ•ˆæœã€‚\n",
      "\n",
      "### æ€ç»´é“¾è¯¦ç»†è¯´æ˜ï¼š\n",
      "1. **æ¡ˆä»¶äº‹å®çš„æ ¸å®**ï¼šäººæ°‘æ£€å¯Ÿé™¢éœ€è¦è¯¦ç»†å®¡æŸ¥æ¡ˆä»¶ä¸­çš„æ¯ä¸€ä¸ªäº‹å®ï¼Œç¡®ä¿æ‰€æœ‰çš„äº‹å®éƒ½å¾—åˆ°äº†å……åˆ†çš„è¯æ˜ï¼Œå¹¶ä¸”ä¸çŠ¯ç½ªè¡Œä¸ºç›´æ¥ç›¸å…³ã€‚è¿™åŒ…æ‹¬å¯¹çŠ¯ç½ªæ„æˆè¦ä»¶çš„åˆ†æï¼Œå¦‚ä¸»ä½“ã€ä¸»è§‚æ–¹é¢ã€å®¢ä½“å’Œå®¢è§‚æ–¹é¢ç­‰ã€‚\n",
      "2. **è¯æ®çš„å……åˆ†æ€§å’Œåˆæ³•æ€§**ï¼šæ‰€æœ‰æäº¤çš„è¯æ®å¿…é¡»ç»è¿‡ä¸¥æ ¼çš„å®¡æŸ¥ï¼Œç¡®ä¿å®ƒä»¬æ˜¯åˆæ³•å–å¾—çš„ï¼Œå¹¶ä¸”èƒ½å¤Ÿå……åˆ†æ”¯æŒæ¡ˆä»¶ä¸­çš„æŒ‡æ§ã€‚è¯æ®ä¸è¶³æˆ–è¯æ®æ¥æºä¸åˆæ³•çš„æ¡ˆä»¶ä¸èƒ½è¢«èµ·è¯‰ã€‚\n",
      "3. **æ³•å¾‹ç¨‹åºçš„å®Œæ•´æ€§**ï¼šå®¡æŸ¥è¿‡ç¨‹ä¸­éœ€è¦ç¡®ä¿æ‰€æœ‰ç›¸å…³çš„æ³•å¾‹ç¨‹åºéƒ½å·²ç»è¢«æ­£ç¡®æ‰§è¡Œï¼ŒåŒ…æ‹¬ä½†ä¸é™äºç«‹æ¡ˆã€ä¾¦æŸ¥ã€è¯æ®æ”¶é›†ç­‰ï¼Œä»¥ç¡®ä¿æ¡ˆä»¶çš„åˆæ³•æ€§ã€‚\n",
      "4. **æ³•å¾‹é€‚ç”¨çš„å‡†ç¡®æ€§**ï¼šç¡®è®¤é€‚ç”¨çš„æ³•å¾‹æ˜¯å¦æ­£ç¡®ï¼ŒåŒ…æ‹¬ç½ªåçš„ç¡®å®šå’Œé‡åˆ‘çš„å»ºè®®æ˜¯å¦åˆç†ï¼Œç¡®ä¿æ³•å¾‹é€‚ç”¨çš„å‡†ç¡®æ— è¯¯ã€‚\n",
      "5. **æ¡ˆä»¶çš„æ³•å¾‹æ•ˆæœå’Œç¤¾ä¼šæ•ˆæœ**ï¼šç»¼åˆè€ƒè™‘æ¡ˆä»¶çš„å¤„ç†å¯¹ç¤¾ä¼šçš„å½±å“ï¼Œç¡®ä¿æ¡ˆä»¶çš„å¤„ç†æ—¢ç¬¦åˆæ³•å¾‹çš„è§„å®šï¼Œåˆèƒ½å¤Ÿå¾—åˆ°ç¤¾ä¼šçš„å¹¿æ³›è®¤å¯å’Œæ”¯æŒï¼Œç»´æŠ¤ç¤¾ä¼šçš„å…¬å¹³æ­£ä¹‰ã€‚\n",
      "\n",
      "### æ€ç»´é“¾æ€»ç»“ï¼š\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶æ—¶ï¼Œå¿…é¡»å…¨é¢æ ¸å®æ¡ˆä»¶äº‹å®ã€è¯æ®ã€æ³•å¾‹ç¨‹åºå’Œæ³•å¾‹é€‚ç”¨ï¼Œç¡®ä¿æ¡ˆä»¶çš„å…¬æ­£æ€§å’Œæ³•å¾‹çš„æ­£ç¡®æ‰§è¡Œã€‚è¿™äº›æ­¥éª¤ä¸ä»…ç¡®ä¿äº†æ¡ˆä»¶çš„è´¨é‡ï¼Œä¹Ÿä¿éšœäº†å½“äº‹äººçš„åˆæ³•æƒç›Šï¼Œä½“ç°äº†æ³•å¾‹çš„ä¸¥è‚ƒæ€§å’Œå…¬æ­£æ€§ã€‚\n",
      "\n",
      "ç°åœ¨ï¼Œè¯·æ ¹æ®ä¸Šè¿°æ€è€ƒè¿‡ç¨‹ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n",
      "\n",
      "é—®é¢˜ï¼šäººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶ï¼Œåº”å½“æŸ¥æ˜å“ªäº›é—®é¢˜ï¼Ÿ\n",
      "\n",
      "å›ç­”ï¼š\n",
      "\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶æ—¶ï¼Œå¿…é¡»ç¡®ä¿æ¡ˆä»¶çš„è¯æ®ç¡®å®å……åˆ†ï¼ŒåŒæ—¶è¦æ ¸å®æ¡ˆæƒ…æ˜¯å¦ç¬¦åˆæ³•å¾‹è§„å®šï¼Œæ­¤å¤–è¿˜è¦æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…è¦çš„æ³•å¾‹ç¨‹åºéƒ½å·²ç»å®Œæˆã€‚è¿™åŒ…æ‹¬å¯¹çŠ¯ç½ªäº‹å®çš„ç¡®è®¤ã€è¯æ®çš„å……åˆ†æ€§å’Œåˆæ³•æ€§ï¼Œä»¥åŠæ³•å¾‹é€‚ç”¨çš„å‡†ç¡®æ€§ã€‚è¿™äº›æ­¥éª¤ç¡®ä¿äº†æ¡ˆä»¶çš„å…¬æ­£æ€§å’Œæ³•å¾‹çš„æ­£ç¡®æ‰§è¡Œã€‚\n",
      "</think>\n",
      "\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶æ—¶ï¼Œåº”å½“æŸ¥æ˜ä»¥ä¸‹é—®é¢˜ï¼š\n",
      "\n",
      "1. **æ¡ˆä»¶äº‹å®çš„æ ¸å®**ï¼šç¡®è®¤æ¡ˆä»¶ä¸­çš„çŠ¯ç½ªäº‹å®æ˜¯å¦æ˜ç¡®ï¼Œæ˜¯å¦æœ‰è¶³å¤Ÿçš„è¯æ®æ”¯æŒã€‚è¿™åŒ…æ‹¬åˆ†æçŠ¯ç½ªæ„æˆè¦ä»¶ï¼Œå¦‚çŠ¯ç½ªä¸»ä½“ã€ä¸»è§‚æ–¹é¢ã€å®¢ä½“å’Œå®¢è§‚æ–¹é¢æ˜¯å¦ç¬¦åˆæ³•å¾‹è§„å®šã€‚\n",
      "\n",
      "2. **è¯æ®çš„å……åˆ†æ€§å’Œåˆæ³•æ€§**ï¼šå®¡æŸ¥æ‰€æœ‰è¯æ®æ˜¯å¦åˆæ³•å–å¾—ï¼Œæ˜¯å¦å……åˆ†æ”¯æŒæŒ‡æ§ï¼Œç¡®ä¿è¯æ®é“¾å®Œæ•´ï¼Œæ²¡æœ‰ç‘•ç–µã€‚\n",
      "\n",
      "3. **æ³•å¾‹ç¨‹åºçš„å®Œæ•´æ€§**ï¼šæ£€æŸ¥æ¡ˆä»¶æ˜¯å¦ç»è¿‡äº†æ­£ç¡®çš„æ³•å¾‹ç¨‹åºï¼Œå¦‚ç«‹æ¡ˆã€ä¾¦æŸ¥ç­‰ï¼Œç¡®ä¿ç¨‹åºåˆæ³•ï¼Œæ²¡æœ‰é—æ¼æˆ–é”™è¯¯ã€‚\n",
      "\n",
      "4. **æ³•å¾‹é€‚ç”¨çš„å‡†ç¡®æ€§**ï¼šç¡®è®¤é€‚ç”¨çš„æ³•å¾‹æ˜¯å¦æ­£ç¡®ï¼Œç½ªåæ˜¯å¦å‡†ç¡®ï¼Œé‡åˆ‘å»ºè®®æ˜¯å¦åˆç†ï¼Œç¡®ä¿æ³•å¾‹é€‚ç”¨æ— è¯¯ã€‚\n",
      "\n",
      "5. **æ¡ˆä»¶çš„æ³•å¾‹æ•ˆæœå’Œç¤¾ä¼šæ•ˆæœ**ï¼šç»¼åˆè€ƒè™‘æ¡ˆä»¶å¤„ç†å¯¹ç¤¾ä¼šçš„å½±å“ï¼Œç¡®ä¿æ¡ˆä»¶æ—¢ç¬¦åˆæ³•å¾‹ï¼Œåˆèƒ½è·å¾—ç¤¾ä¼šè®¤å¯ï¼Œç»´æŠ¤å…¬å¹³æ­£ä¹‰ã€‚\n",
      "\n",
      "é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œäººæ°‘æ£€å¯Ÿé™¢ç¡®ä¿æ¡ˆä»¶è´¨é‡ï¼Œä¿éšœå½“äº‹äººæƒç›Šï¼Œä½“ç°æ³•å¾‹çš„ä¸¥è‚ƒæ€§å’Œå…¬æ­£æ€§ã€‚<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "question = \"äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶ï¼Œåº”å½“æŸ¥æ˜å“ªäº›é—®é¢˜ï¼Ÿ\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_style.format(question)], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=8192,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### å“åº”ï¼š\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ®µç­”è¾©çš„è¾©è®ºæé—®ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿é€»è¾‘å’Œå‡†ç¡®çš„å“åº”ã€‚\n",
    "\n",
    "### æŒ‡ä»¤ï¼š\n",
    "ä½ æ˜¯ä¸€ä½åœ¨ä¸­å›½æ³•å¾‹ã€æ¡ˆä¾‹æ¨ç†å’Œåˆ¤å†³æ–¹é¢å…·æœ‰é«˜çº§çŸ¥è¯†çš„æ³•å¾‹ä¸“å®¶ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹æ³•å¾‹é—®é¢˜ã€‚\n",
    "\n",
    "### é—®é¢˜ï¼š\n",
    "{}\n",
    "\n",
    "### å“åº”ï¼š\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é›†æ„å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "# è¿­ä»£è®­ç»ƒé›†æ•°æ®ï¼Œå¤„ç†prompt\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"question\"]\n",
    "    cots = examples[\"reasoning\"]\n",
    "    outputs = examples[\"response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'reasoning', 'response']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", 'zh', split = \"train[0:500]\", trust_remote_code=True)\n",
    "dataset = load_dataset(\"json\", data_files=\"law_data.json\", split=\"train\", trust_remote_code=True)\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»¥ä¸‹æ˜¯ä¸€æ®µç­”è¾©çš„è¾©è®ºæé—®ã€‚\n",
      "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿é€»è¾‘å’Œå‡†ç¡®çš„å“åº”ã€‚\n",
      "\n",
      "### æŒ‡ä»¤ï¼š\n",
      "ä½ æ˜¯ä¸€ä½åœ¨ä¸­å›½æ³•å¾‹ã€æ¡ˆä¾‹æ¨ç†å’Œåˆ¤å†³æ–¹é¢å…·æœ‰é«˜çº§çŸ¥è¯†çš„æ³•å¾‹ä¸“å®¶ã€‚\n",
      "è¯·å›ç­”ä»¥ä¸‹æ³•å¾‹é—®é¢˜ã€‚\n",
      "\n",
      "### é—®é¢˜ï¼š\n",
      "äº¤é€šè‚‡äº‹ç½ªçš„æ³•å®šåˆ‘ç½šæ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "\n",
      "### å“åº”ï¼š\n",
      "<think>\n",
      "æ ¹æ®åˆ‘æ³•ç¬¬133æ¡ï¼Œäº¤é€šè‚‡äº‹ç½ªå› æƒ…èŠ‚å’Œåæœä¸åŒï¼Œæ³•å®šåˆ‘ç½šä¹Ÿæœ‰æ‰€ä¸åŒã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘æˆ–è€…æ‹˜å½¹ï¼›å¦‚æœäº¤é€šè¿è¾“è‚‡äº‹åé€ƒé€¸æˆ–æœ‰å…¶ä»–ç‰¹åˆ«æ¶åŠ£æƒ…èŠ‚ï¼Œåˆ™å¤„ä¸‰å¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ï¼›å› é€ƒé€¸è‡´äººæ­»äº¡çš„ï¼Œå¤„ä¸ƒå¹´ä»¥ä¸Šæœ‰æœŸå¾’åˆ‘ã€‚\n",
      "</think>\n",
      "äº¤é€šè‚‡äº‹ç½ªä¸€èˆ¬å¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘æˆ–è€…æ‹˜å½¹ï¼›å¦‚æœäº¤é€šè¿è¾“è‚‡äº‹åé€ƒé€¸æˆ–æœ‰å…¶ä»–ç‰¹åˆ«æ¶åŠ£æƒ…èŠ‚ï¼Œåˆ™å¤„ä¸‰å¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ï¼›å› é€ƒé€¸è‡´äººæ­»äº¡çš„ï¼Œå¤„ä¸ƒå¹´ä»¥ä¸Šæœ‰æœŸå¾’åˆ‘ã€‚<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "print(dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.12 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    # éœ€è¦åº”ç”¨LoRAçš„ç›®æ ‡æ¨¡å—ç¥ç»ç½‘ç»œå±‚\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attentionç›¸å…³å±‚\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],   # Feed Forwardç›¸å…³å±‚\n",
    "    lora_alpha = 16,  # LoRAç¼©æ”¾å› å­ï¼Œç”¨äºæ§åˆ¶LoRAçš„æ›´æ–°å¹…åº¦ã€‚å€¼è¶Šå¤§ï¼ŒLoRAæ›´æ–°çš„å½±å“è¶Šå¤§ã€‚\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized / LoRAå±‚çš„Dropoutç‡ï¼Œè®¾ç½®ä¸º0è¡¨ç¤ºä¸ä½¿ç”¨\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 4321,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:00<00:00, 301.79 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [00:00<00:00, 1013.65 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 258 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 80\n",
      " \"-____-\"     Number of trainable parameters = 68,812,800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 08:19, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.874100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.643100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.648200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model, \n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset, \n",
    "    dataset_text_field = \"text\", # æ•°æ®é›†å­—æ®µåç§°\n",
    "    max_seq_length = max_seq_length, # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    dataset_num_proc = 2, # å¤„ç†æ•°æ®é›†çš„å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œæé«˜CPUçš„ä½¿ç”¨ç‡\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4, # æ¯ä¸ªGPUè®­ç»ƒæ‰¹æ¬¡çš„å¤§å°\n",
    "        gradient_accumulation_steps = 4, # æ¢¯åº¦ç´¯è®¡æ­¥æ•°ï¼Œç”¨äºæ¨¡æ‹Ÿæ›´å¤§çš„batch size\n",
    "        warmup_steps = 5, # é¢„çƒ­æ­¥æ•°\n",
    "        max_steps = 80, # æœ€å¤§æ­¥æ•°ï¼ˆä¸€æ­¥ = å¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®ï¼‰\n",
    "        # num_train_epochs = 1, # For longer training runs!\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(), \n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5, # æ¯5æ­¥è®°å½•ä¸€ä¸‹æ—¥å¿— Loss ä¿¡æ¯\n",
    "        optim = \"adamw_8bit\", # ä¼˜åŒ–å™¨\n",
    "        weight_decay = 0.01, # æƒé‡è¡°å‡ç³»æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        lr_scheduler_type = \"linear\", # å­¦ä¹ ç‡è§„åˆ’å™¨ï¼Œå­¦ä¹ ç‡çº¿æ€§è¡°å‡\n",
    "        seed = 4321,\n",
    "        output_dir = \"outputs\", # checkpointè¾“å‡ºä¿å­˜è·¯å¾„æ–‡ä»¶å¤¹\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "# è®­ç»ƒ\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œbeginâ–ofâ–sentenceï½œ>ä»¥ä¸‹æ˜¯ä¸€æ®µç­”è¾©çš„è¾©è®ºæé—®ã€‚\n",
      "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿é€»è¾‘å’Œå‡†ç¡®çš„å“åº”ã€‚\n",
      "\n",
      "### æŒ‡ä»¤ï¼š\n",
      "ä½ æ˜¯ä¸€ä½åœ¨ä¸­å›½æ³•å¾‹ã€æ¡ˆä¾‹æ¨ç†å’Œåˆ¤å†³æ–¹é¢å…·æœ‰é«˜çº§çŸ¥è¯†çš„æ³•å¾‹ä¸“å®¶ã€‚\n",
      "è¯·å›ç­”ä»¥ä¸‹æ³•å¾‹é—®é¢˜ã€‚\n",
      "\n",
      "### é—®é¢˜ï¼š\n",
      "äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶ï¼Œåº”å½“æŸ¥æ˜å“ªäº›é—®é¢˜ï¼Ÿ\n",
      "\n",
      "### å“åº”ï¼š\n",
      "<think>\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶æ—¶ï¼Œéœ€è¦å…¨é¢å®¡æŸ¥æ¡ˆä»¶çš„äº‹å®ã€è¯æ®ã€ç¨‹åºç­‰å„ä¸ªæ–¹é¢ï¼Œä»¥ç¡®ä¿æ¡ˆä»¶ç¬¦åˆèµ·è¯‰æ¡ä»¶ã€‚å…·ä½“åŒ…æ‹¬çŠ¯ç½ªäº‹å®æ˜¯å¦æ¸…æ¥šï¼Œè¯æ®æ˜¯å¦ç¡®å®ã€å……åˆ†ï¼Œæ˜¯å¦ä¾æ³•æ’é™¤éæ³•è¯æ®ï¼Œæ˜¯å¦ç¬¦åˆæ³•å®šèµ·è¯‰æ¡ä»¶ç­‰ã€‚\n",
      "</think>\n",
      "äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶ï¼Œåº”å½“æŸ¥æ˜ï¼šï¼ˆä¸€ï¼‰çŠ¯ç½ªå«Œç–‘äººèº«ä»½çŠ¶å†µæ˜¯å¦æ¸…æ¥šï¼ŒåŒ…æ‹¬å§“åã€æ€§åˆ«ã€å›½ç±ã€å‡ºç”Ÿå¹´æœˆæ—¥ã€èŒä¸šã€å•ä½ã€ä½å¤„ã€æ˜¯å¦å—è¿‡åˆ‘äº‹å¤„åˆ†ç­‰ï¼›ï¼ˆäºŒï¼‰çŠ¯ç½ªäº‹å®æ˜¯å¦æ¸…æ¥šï¼ŒåŒ…æ‹¬çŠ¯ç½ªæ—¶é—´ã€åœ°ç‚¹ã€æ‰‹æ®µã€å±å®³åæœã€çŠ¯ç½ªå«Œç–‘äººç½ªè´£ç­‰ï¼›ï¼ˆä¸‰ï¼‰è®¤å®šçŠ¯ç½ªæ€§è´¨å’Œç½ªåçš„æ„è§æ˜¯å¦æ­£ç¡®ï¼›ï¼ˆå››ï¼‰æœ‰æ— é—æ¼ç½ªè¡Œå’Œå…¶ä»–åº”å½“è¿½ç©¶åˆ‘äº‹è´£ä»»çš„äººï¼›ï¼ˆäº”ï¼‰æ˜¯å¦å±äºä¸è¿½ç©¶åˆ‘äº‹è´£ä»»çš„æ³•å®šæƒ…å½¢ï¼›ï¼ˆå…­ï¼‰æœ‰æ— é™„å¸¦æ°‘äº‹è¯‰è®¼ï¼›ï¼ˆä¸ƒï¼‰ä¾¦æŸ¥æ´»åŠ¨æ˜¯å¦åˆæ³•ã€‚<ï½œendâ–ofâ–sentenceï½œ>\n",
      "\n",
      "äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶ï¼Œåº”å½“æŸ¥æ˜å“ªäº›é—®é¢˜ï¼Ÿ\n",
      "\n",
      "\n",
      "\n",
      "<think>\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶æ—¶ï¼Œéœ€è¦å…¨é¢å®¡æŸ¥æ¡ˆä»¶çš„äº‹å®ã€è¯æ®ã€ç¨‹åºç­‰å„ä¸ªæ–¹é¢ï¼Œä»¥ç¡®ä¿æ¡ˆä»¶ç¬¦åˆèµ·è¯‰æ¡ä»¶ã€‚å…·ä½“åŒ…æ‹¬çŠ¯ç½ªäº‹å®æ˜¯å¦æ¸…æ¥šï¼Œè¯æ®æ˜¯å¦ç¡®å®ã€å……åˆ†ï¼Œæ˜¯å¦ä¾æ³•æ’é™¤éæ³•è¯æ®ï¼Œæ˜¯å¦ç¬¦åˆæ³•å®šèµ·è¯‰æ¡ä»¶ç­‰ã€‚\n",
      "</think>\n",
      "äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„æ¡ˆä»¶ï¼Œåº”å½“æŸ¥æ˜ï¼šï¼ˆä¸€ï¼‰çŠ¯ç½ªå«Œç–‘äººèº«ä»½çŠ¶å†µæ˜¯å¦æ¸…æ¥šï¼ŒåŒ…æ‹¬å§“åã€æ€§åˆ«ã€å›½ç±ã€å‡ºç”Ÿå¹´æœˆæ—¥ã€èŒä¸šã€å•ä½ã€ä½å¤„ã€æ˜¯å¦å—è¿‡åˆ‘äº‹å¤„åˆ†ç­‰ï¼›ï¼ˆäºŒï¼‰çŠ¯ç½ªäº‹å®æ˜¯å¦æ¸…æ¥šï¼ŒåŒ…æ‹¬çŠ¯ç½ªæ—¶é—´ã€åœ°ç‚¹ã€æ‰‹æ®µã€å±å®³åæœã€çŠ¯ç½ªå«Œç–‘äººç½ªè´£ç­‰ï¼›ï¼ˆä¸‰ï¼‰è®¤å®šçŠ¯ç½ªæ€§è´¨å’Œç½ªåçš„æ„è§æ˜¯å¦æ­£ç¡®ï¼›ï¼ˆå››ï¼‰æœ‰æ— é—æ¼ç½ªè¡Œå’Œå…¶ä»–åº”å½“è¿½ç©¶åˆ‘äº‹è´£ä»»çš„äººï¼›ï¼ˆäº”ï¼‰æ˜¯å¦å±äºä¸è¿½ç©¶åˆ‘äº‹è´£ä»»çš„æ³•å®šæƒ…å½¢ï¼›ï¼ˆå…­ï¼‰æœ‰æ— é™„å¸¦æ°‘äº‹è¯‰è®¼ï¼›ï¼ˆä¸ƒï¼‰ä¾¦æŸ¥æ´»åŠ¨æ˜¯å¦åˆæ³•ã€‚<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question)], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=8192,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])\n",
    "print(response[0].split(\"### é—®é¢˜ï¼š\")[1].split(\"### å“åº”ï¼š\")[0])\n",
    "print(response[0].split(\"### å“åº”ï¼š\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ï½œbeginâ–ofâ–sentenceï½œ>ä»¥ä¸‹æ˜¯ä¸€æ®µç­”è¾©çš„è¾©è®ºæé—®ã€‚\n",
      "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€æ­¥çš„æ€ç»´é“¾ï¼Œä»¥ç¡®ä¿é€»è¾‘å’Œå‡†ç¡®çš„å“åº”ã€‚\n",
      "\n",
      "### æŒ‡ä»¤ï¼š\n",
      "ä½ æ˜¯ä¸€ä½åœ¨ä¸­å›½æ³•å¾‹ã€æ¡ˆä¾‹æ¨ç†å’Œåˆ¤å†³æ–¹é¢å…·æœ‰é«˜çº§çŸ¥è¯†çš„æ³•å¾‹ä¸“å®¶ã€‚\n",
      "è¯·å›ç­”ä»¥ä¸‹æ³•å¾‹é—®é¢˜ã€‚\n",
      "\n",
      "### é—®é¢˜ï¼š\n",
      "å¦‚æœä¸€èµ·æ¡ˆä»¶æ˜¯ç”±äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„ï¼Œéœ€è¦æŸ¥æ˜å“ªäº›é—®é¢˜ï¼Ÿ\n",
      "\n",
      "### å“åº”ï¼š\n",
      "<think>\n",
      "æ ¹æ®åˆ‘äº‹è¯‰è®¼æ³•çš„è§„å®šï¼Œäººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰æ—¶ï¼Œéœ€è¦å¯¹æ¡ˆä»¶è¿›è¡Œå…¨é¢å®¡æŸ¥ï¼ŒåŒ…æ‹¬çŠ¯ç½ªäº‹å®ã€è¯æ®ã€ç¨‹åºã€æ³•å¾‹é€‚ç”¨ç­‰æ–¹é¢ã€‚å…·ä½“åŒ…æ‹¬å®¡æŸ¥çŠ¯ç½ªå«Œç–‘äººæ˜¯å¦ç¬¦åˆèµ·è¯‰æ¡ä»¶ï¼Œæ˜¯å¦çŠ¯ç½ªäº‹å®æ¸…æ¥šï¼Œè¯æ®ç¡®å®ã€å……åˆ†ï¼Œæ˜¯å¦éœ€è¦å¯¹ä¾¦æŸ¥æ´»åŠ¨è¿›è¡Œç›‘ç£ï¼Œä»¥åŠæ˜¯å¦éœ€è¦é€€å›è¡¥å……ä¾¦æŸ¥ç­‰ã€‚\n",
      "</think>\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰æ—¶ï¼Œåº”æŸ¥æ˜ä»¥ä¸‹é—®é¢˜ï¼š(ä¸€)çŠ¯ç½ªå«Œç–‘äººèº«ä»½çŠ¶å†µæ˜¯å¦æ¸…æ¥šï¼Œæ˜¯å¦ç¬¦åˆèµ·è¯‰æ¡ä»¶ï¼›(äºŒ)çŠ¯ç½ªäº‹å®ã€æƒ…èŠ‚æ˜¯å¦æ¸…æ¥šï¼Œæ˜¯å¦ç¬¦åˆèµ·è¯‰æ¡ä»¶ï¼›(ä¸‰)ä¾¦æŸ¥æ´»åŠ¨æ˜¯å¦åˆæ³•ï¼Œæ˜¯å¦éœ€è¦çº æ­£ï¼›(å››)æ˜¯å¦éœ€è¦å¯¹ä¾¦æŸ¥æ´»åŠ¨è¿›è¡Œç›‘ç£ï¼›(äº”)çŠ¯ç½ªå«Œç–‘äººæ˜¯å¦è®¤ç½ªè®¤ç½šï¼Œæ˜¯å¦éœ€è¦ç­¾ç½²è®¤ç½ªè®¤ç½šå…·ç»“ä¹¦ï¼›(å…­)æ˜¯å¦éœ€è¦é€€å›è¡¥å……ä¾¦æŸ¥ï¼›(ä¸ƒ)å…¶ä»–éœ€è¦æŸ¥æ˜çš„äº‹é¡¹ã€‚<ï½œendâ–ofâ–sentenceï½œ>\n",
      "\n",
      "å¦‚æœä¸€èµ·æ¡ˆä»¶æ˜¯ç”±äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„ï¼Œéœ€è¦æŸ¥æ˜å“ªäº›é—®é¢˜ï¼Ÿ\n",
      "\n",
      "\n",
      "\n",
      "<think>\n",
      "æ ¹æ®åˆ‘äº‹è¯‰è®¼æ³•çš„è§„å®šï¼Œäººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰æ—¶ï¼Œéœ€è¦å¯¹æ¡ˆä»¶è¿›è¡Œå…¨é¢å®¡æŸ¥ï¼ŒåŒ…æ‹¬çŠ¯ç½ªäº‹å®ã€è¯æ®ã€ç¨‹åºã€æ³•å¾‹é€‚ç”¨ç­‰æ–¹é¢ã€‚å…·ä½“åŒ…æ‹¬å®¡æŸ¥çŠ¯ç½ªå«Œç–‘äººæ˜¯å¦ç¬¦åˆèµ·è¯‰æ¡ä»¶ï¼Œæ˜¯å¦çŠ¯ç½ªäº‹å®æ¸…æ¥šï¼Œè¯æ®ç¡®å®ã€å……åˆ†ï¼Œæ˜¯å¦éœ€è¦å¯¹ä¾¦æŸ¥æ´»åŠ¨è¿›è¡Œç›‘ç£ï¼Œä»¥åŠæ˜¯å¦éœ€è¦é€€å›è¡¥å……ä¾¦æŸ¥ç­‰ã€‚\n",
      "</think>\n",
      "äººæ°‘æ£€å¯Ÿé™¢åœ¨å®¡æŸ¥ç§»é€èµ·è¯‰æ—¶ï¼Œåº”æŸ¥æ˜ä»¥ä¸‹é—®é¢˜ï¼š(ä¸€)çŠ¯ç½ªå«Œç–‘äººèº«ä»½çŠ¶å†µæ˜¯å¦æ¸…æ¥šï¼Œæ˜¯å¦ç¬¦åˆèµ·è¯‰æ¡ä»¶ï¼›(äºŒ)çŠ¯ç½ªäº‹å®ã€æƒ…èŠ‚æ˜¯å¦æ¸…æ¥šï¼Œæ˜¯å¦ç¬¦åˆèµ·è¯‰æ¡ä»¶ï¼›(ä¸‰)ä¾¦æŸ¥æ´»åŠ¨æ˜¯å¦åˆæ³•ï¼Œæ˜¯å¦éœ€è¦çº æ­£ï¼›(å››)æ˜¯å¦éœ€è¦å¯¹ä¾¦æŸ¥æ´»åŠ¨è¿›è¡Œç›‘ç£ï¼›(äº”)çŠ¯ç½ªå«Œç–‘äººæ˜¯å¦è®¤ç½ªè®¤ç½šï¼Œæ˜¯å¦éœ€è¦ç­¾ç½²è®¤ç½ªè®¤ç½šå…·ç»“ä¹¦ï¼›(å…­)æ˜¯å¦éœ€è¦é€€å›è¡¥å……ä¾¦æŸ¥ï¼›(ä¸ƒ)å…¶ä»–éœ€è¦æŸ¥æ˜çš„äº‹é¡¹ã€‚<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "question = \"å¦‚æœä¸€èµ·æ¡ˆä»¶æ˜¯ç”±äººæ°‘æ£€å¯Ÿé™¢å®¡æŸ¥ç§»é€èµ·è¯‰çš„ï¼Œéœ€è¦æŸ¥æ˜å“ªäº›é—®é¢˜ï¼Ÿ\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(question)], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=8192,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0])\n",
    "print(response[0].split(\"### é—®é¢˜ï¼š\")[1].split(\"### å“åº”ï¼š\")[0])\n",
    "print(response[0].split(\"### å“åº”ï¼š\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 68.37 out of 125.49 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 12/48 [00:00<00:00, 37.24it/s]\n",
      "We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:02<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "new_sft_model = \"DeepSeek-R1-Law-CoT-14B\"\n",
    "model.save_pretrained(new_sft_model) # Local saving\n",
    "tokenizer.save_pretrained(new_sft_model)\n",
    "model.save_pretrained_merged(new_sft_model, tokenizer, save_method = \"merged_16bit\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "law_sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
